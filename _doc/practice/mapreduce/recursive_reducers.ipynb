{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducers récursifs\n",
    "\n",
    "J'utilise volontiers une terminologie découverte chez Microsoft pour illustrer une façon d'écrire le même calcul qui a un impact sur la facilité avec laquelle on peut le distribution : utiliser des comptes plutôt que des moyennes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le notebook utilise des fonctions développées pour illustrer les notions, plus claires qu'efficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream\n",
    "\n",
    "Le map reduce s'applique à des jeux de données très grands. D'un point de vue mathématique, on écrit des algorithmes qui s'appliquent à des jeux de données infinis ou plutôt dont la taille n'est pas connu. Pour les distinguer des jeux de données, on les appelle des *flux* ou *stream* en anglais.\n",
    "\n",
    "En aparté, écrits pour être parallélisés, ces traitements ont la particuliarité de ne pas conserver l'ordre dans lequel il traite les données. C'est particulièrement vrai lorsque le jeu de données est divisé sur plusieurs disques durs. Il est impossible de choisir un morceau en premier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper\n",
    "\n",
    "Un *mapper* applique le même traitement à chaque observation du *stream* de façon indépendante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = [(\"a\", 1), (\"b\", 4), (\"a\", 6), (\"a\", 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f0e1b597fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from teachcompute.fctmr import mapper\n",
    "\n",
    "stream1 = mapper(lambda el: (el[0], el[1] + 1), ens)\n",
    "stream1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat n'existe pas tant qu'on ne demande explicitement que le calcul soit faut. Il faut parcourir le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 5), ('a', 7), ('a', 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stream1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on ne peut le parcourir qu'une fois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stream1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coût du premier élément\n",
    "\n",
    "Quand on a une infinité d'éléments à traiter, il est important de pouvoir regarder ce qu'un traitement donne sur les premiers éléments. Avec un mapper, cela correspond au coût d'un seul map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teachcompute.fctmr import take\n",
    "\n",
    "first = lambda it: take(it, count=1)\n",
    "big_ens = ens * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.5 µs ± 13.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 list(mapper(lambda el: (el[0], el[1]+1), big_ens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834 ns ± 574 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 first(mapper(lambda el: (el[0], el[1]+1), big_ens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "Un vrai *reducer* réduit les éléments d'un ensemble, il ne répartit pas les données. En pratique, on réduit rarement un ensemble qu'on n'a pas distribué au préalable, comme avec un *groupby*. On ne réduit pas toujours non plus un ensemble à une seule ligne. On empile les opérations de streaming, on repousse également le moment d'évaluer. La distribution s'effectue selon une clé qui est hashée (voir [Hash et distribution](https://sdpython.github.io/doc/teachcompute/dev/practice/expose/hash_distribution.html)). La première lambda fonction décrit ce qu'est cette clé, le premier élément du couple dans ce cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object reducer at 0x7f0e1b5dba70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from teachcompute.fctmr import reducer\n",
    "\n",
    "stream1 = mapper(lambda el: (el[0], el[1] + 1), ens)\n",
    "stream2 = reducer(lambda el: el[0], stream1, asiter=False)\n",
    "stream2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [('a', 2), ('a', 4), ('a', 7)]), ('b', [('b', 5)])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, le *reducer* réduit chaque groupe à un seul résultat qui est l'ensemble des éléments. Quel est le coup du premier élément..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35 µs ± 841 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def test2(ens, one=False):\n",
    "    stream1 = mapper(lambda el: (el[0], el[1] + 1), ens)\n",
    "    stream2 = reducer(lambda el: el[0], stream1, asiter=False)\n",
    "    return list(stream2) if one else first(stream2)\n",
    "\n",
    "\n",
    "%timeit -n 1000 test2(big_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 µs ± 30.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 test2(big_ens, one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est plus court mais pas significativement plus court. Cela correspond au coût d'un tri de l'ensemble des observations et du coût de la construction du premier groupe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer et tri\n",
    "\n",
    "Un stream est infini en théorie. En pratique il est fini mais on ne sait pas si un ou plusieurs groupes entiers tiendraient en mémoire. Une façon de faire est de limiter la présence des données en mémoire à un seul groupe et pour cela, il faut d'abord trier les données selon les clés. Ce n'est pas indispensable mais dans le pire des cas, c'est une bonne option. On pourrait avoir un stream comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0),\n",
       " ('b', 1),\n",
       " ('c', 2),\n",
       " ('d', 3),\n",
       " ('e', 4),\n",
       " ('f', 5),\n",
       " ('g', 6),\n",
       " ('h', 7),\n",
       " ('g', 8),\n",
       " ('f', 9),\n",
       " ('e', 10),\n",
       " ('d', 11),\n",
       " ('c', 12),\n",
       " ('b', 13),\n",
       " ('a', 14)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pas_cool = [(chr(int(c) + 96), i) for i, c in enumerate(str(11111111**2))]\n",
    "pas_cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le groupe *a* est au début et à la fin, si on regroupe en mémoire, le groupe associé à *a* doit rester en mémoire du début à la fin. On ne sait jamais si un groupe ne va pas réapparaître plus tard. En triant, on est sûr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un autre map\n",
    "\n",
    "On ajoute un dernier map qui fait la somme des éléments de chaque groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f0e1b597580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_gr(key_gr):\n",
    "    key, gr = key_gr\n",
    "    return key, sum(e[1] for e in gr)\n",
    "\n",
    "\n",
    "stream1 = mapper(lambda el: (el[0], el[1] + 1), ens)\n",
    "stream2 = reducer(lambda el: el[0], stream1)\n",
    "stream3 = map(sum_gr, stream2)\n",
    "stream3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 13), ('b', 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stream3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combiner ou join\n",
    "\n",
    "Un *combiner* ou *join* permet de fusionner deux bases de données qui ont en commun une clé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object combiner at 0x7f0e1b5dba00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from teachcompute.fctmr import combiner\n",
    "\n",
    "stream1 = mapper(lambda el: (el[0], el[1] + 1), ens)\n",
    "stream2 = reducer(lambda el: el[0], stream1)\n",
    "stream3 = map(sum_gr, stream2)\n",
    "stream4 = mapper(lambda el: (el[0], el[1] + 10), pas_cool)\n",
    "comb = combiner(lambda el: el[0], stream3, lambda el: el[0], stream4)\n",
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 13), ('a', 10)),\n",
       " (('a', 13), ('a', 24)),\n",
       " (('b', 5), ('b', 11)),\n",
       " (('b', 5), ('b', 23))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le coût du premier élément est un peu plus compliqué à inférer, cela dépend beaucoup des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.72 µs ± 1.26 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def job(ens, ens2, one=False, sens=True):\n",
    "    stream1 = mapper(lambda el: (el[0], el[1] + 1), ens)\n",
    "    stream2 = reducer(lambda el: el[0], stream1)\n",
    "    stream3 = map(sum_gr, stream2)\n",
    "    stream4 = mapper(lambda el: (el[0], el[1] + 10), ens2)\n",
    "    if sens:\n",
    "        comb = combiner(lambda el: el[0], stream3, lambda el: el[0], stream4)\n",
    "    else:\n",
    "        comb = combiner(lambda el: el[0], stream4, lambda el: el[0], stream3)\n",
    "    return list(comb) if one else first(comb)\n",
    "\n",
    "\n",
    "%timeit -n 1000 job(big_ens, pas_cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41 µs ± 1.82 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 job(big_ens, pas_cool, sens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 µs ± 35.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 job(big_ens, pas_cool, one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 µs ± 22.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 job(big_ens, pas_cool, one=True, sens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a différentes façons de coder un *combiner*, l'une d'elle consiste à réduire chacun des deux streams puis à faire le produit croisé de chaque groupe assemblé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducers récursifs\n",
    "\n",
    "C'est pas loin d'être un abus de langage, disons que cela réduit la dépendance au tri. Un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 10), ('b', 4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_gr(key_gr):\n",
    "    key, gr = key_gr\n",
    "    return key, sum(e[1] for e in gr)\n",
    "\n",
    "\n",
    "def job_recursif(ens):\n",
    "    stream2 = reducer(lambda el: el[0], ens)\n",
    "    stream3 = map(sum_gr, stream2)\n",
    "    return list(stream3)\n",
    "\n",
    "\n",
    "job_recursif(ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant, on coupe en deux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(ens) // 2\n",
    "job_recursif(ens[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 9)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_recursif(ens[n:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 10), ('b', 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_recursif(job_recursif(ens[:n]) + job_recursif(ens[n:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le job ainsi écrit est associatif en quelque sorte. Cela laisse plus de liberté pour la distribution car on peut maintenant distribuer des clés identiques sur des machines différentes puis réappliquer le *reducer* sur les résultats de la première salve. C'est d'autant plus efficace que le *reducer* réduit beaucoup les données. Il reste à voir le cas d'un *reducer* **non récursif**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3.3333333333333335), ('b', 4.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(ens):\n",
    "    s = 0.0\n",
    "    for i, e in enumerate(ens):\n",
    "        s += e\n",
    "    return s / (i + 1)\n",
    "\n",
    "\n",
    "def mean_gr(key_gr):\n",
    "    key, gr = key_gr\n",
    "    return key, mean(e[1] for e in gr)\n",
    "\n",
    "\n",
    "def job_non_recursif(ens):\n",
    "    stream2 = reducer(lambda el: el[0], ens)\n",
    "    stream3 = map(mean_gr, stream2)\n",
    "    return list(stream3)\n",
    "\n",
    "\n",
    "job_non_recursif(ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1.0), ('b', 4.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(ens) // 2\n",
    "job_non_recursif(ens[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4.5)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_non_recursif(ens[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2.75), ('b', 4.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_non_recursif(job_non_recursif(ens[:n]) + job_non_recursif(ens[n:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce *job* ne doit pas être distribué n'importe comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}