{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices en 3 colonnes\n",
    "\n",
    "Représentation d'une matrice avec Spark / Map / Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook propose d'implémenter un produit matriciel sous Spark. Spark comme SQL n'aime pas trop avoir un nombre de colonnes variables. La première étape consiste à transformer les matrices $I\\times J$ en tableau de trois colonnes $(i,j,coefficient)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session spark with no cluster\n",
    "\n",
    "Spark est censé tourner sur un cluster. Mais ce n'est pas essentielle pour comprendre la logique. Le notebook tourne donc en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/04 22:03:48 WARN Utils: Your hostname, xadupre2025, resolves to a loopback address: 127.0.1.1; using 172.17.197.78 instead (on interface eth0)\n",
      "25/11/04 22:03:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/04 22:03:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une matrice aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.82769564, 4.3446237 ],\n",
       "       [2.70903159, 4.28609675],\n",
       "       [2.3197026 , 4.01486866],\n",
       "       [2.61458882, 4.66117542],\n",
       "       [1.99321382, 3.49690316],\n",
       "       [3.08580871, 3.67840109],\n",
       "       [2.99800173, 4.73972669],\n",
       "       [3.55996446, 4.85961952],\n",
       "       [2.51579451, 3.26530604],\n",
       "       [2.21882233, 3.76041136]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import rand\n",
    "\n",
    "rnd1 = rand(10, 10)\n",
    "rnd2 = rand(10, 2)\n",
    "rnd1 @ rnd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.452410</td>\n",
       "      <td>0.818571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.384679</td>\n",
       "      <td>0.965623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.646845</td>\n",
       "      <td>0.539604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138227</td>\n",
       "      <td>0.917017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.422069</td>\n",
       "      <td>0.801644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.238986</td>\n",
       "      <td>0.595039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.895846</td>\n",
       "      <td>0.681612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.410572</td>\n",
       "      <td>0.862415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.869270</td>\n",
       "      <td>0.509816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.588944</td>\n",
       "      <td>0.955183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.452410  0.818571\n",
       "1  0.384679  0.965623\n",
       "2  0.646845  0.539604\n",
       "3  0.138227  0.917017\n",
       "4  0.422069  0.801644\n",
       "5  0.238986  0.595039\n",
       "6  0.895846  0.681612\n",
       "7  0.410572  0.862415\n",
       "8  0.869270  0.509816\n",
       "9  0.588944  0.955183"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df1 = pandas.DataFrame(rnd1)\n",
    "df2 = pandas.DataFrame(rnd2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"rnd1.txt\", sep=\"\\t\", header=None, index=False)\n",
    "df2.to_csv(\"rnd2.txt\", sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = spark.sparkContext.textFile(\"rnd1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion d'une matrice au format Spark\n",
    "\n",
    "Lorsqu'un traitement est distribué en Map/Reduce, il n'est pas possible de s'appuyer sur l'ordre dans lequel sont traitées les lignes. Le plus est d'ajouter cette information sur chaque ligne plutôt que de chercher à la récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"rnd1.txt\", sep=\"\\t\", header=None, index=True)\n",
    "df2.to_csv(\"rnd2.txt\", sep=\"\\t\", header=None, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_mat_row(row):\n",
    "    values = row.split(\"\\t\")\n",
    "    index = int(values[0])\n",
    "    values = [float(_) for _ in values[1:]]\n",
    "    return [[index, j, v] for j, v in enumerate(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.03659454680020524],\n",
       " [0, 1, 0.767607811315502],\n",
       " [0, 2, 0.5733073849596189],\n",
       " [0, 3, 0.8757599092593937],\n",
       " [0, 4, 0.9367553854443735],\n",
       " [0, 5, 0.41209851669976916],\n",
       " [0, 6, 0.5540866293161243],\n",
       " [0, 7, 0.4665984547370805],\n",
       " [0, 8, 0.7567418978844924],\n",
       " [0, 9, 0.3129986409413059],\n",
       " [1, 0, 0.49628400193691236],\n",
       " [1, 1, 0.4944250595188363]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = sc.textFile(\"rnd1.txt\")\n",
    "new_mat1 = mat1.flatMap(process_mat_row)\n",
    "new_mat1.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.452409743518958],\n",
       " [0, 1, 0.818571020089081],\n",
       " [1, 0, 0.3846793223675351],\n",
       " [1, 1, 0.9656232217356503],\n",
       " [2, 0, 0.6468448619803056],\n",
       " [2, 1, 0.5396040456507285],\n",
       " [3, 0, 0.13822741516437753],\n",
       " [3, 1, 0.9170173659401171],\n",
       " [4, 0, 0.4220691346862584],\n",
       " [4, 1, 0.8016438317882735],\n",
       " [5, 0, 0.23898597387372977],\n",
       " [5, 1, 0.5950391411819209]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = sc.textFile(\"rnd2.txt\")\n",
    "new_mat2 = mat2.flatMap(process_mat_row)\n",
    "new_mat2.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produit matriciel\n",
    "\n",
    "Il faut d'abord faire la jointure avec la méthode [join](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join). Il faut que la clé soit sur la première colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ((0, 0.03659454680020524), (0, 0.452409743518958))),\n",
       " (0, ((0, 0.03659454680020524), (1, 0.818571020089081))),\n",
       " (0, ((1, 0.49628400193691236), (0, 0.452409743518958))),\n",
       " (0, ((1, 0.49628400193691236), (1, 0.818571020089081))),\n",
       " (0, ((2, 0.26823832256619184), (0, 0.452409743518958))),\n",
       " (0, ((2, 0.26823832256619184), (1, 0.818571020089081))),\n",
       " (0, ((3, 0.8516746510081805), (0, 0.452409743518958))),\n",
       " (0, ((3, 0.8516746510081805), (1, 0.818571020089081))),\n",
       " (0, ((4, 0.210077377768215), (0, 0.452409743518958))),\n",
       " (0, ((4, 0.210077377768215), (1, 0.818571020089081))),\n",
       " (0, ((5, 0.0024981117896399896), (0, 0.452409743518958))),\n",
       " (0, ((5, 0.0024981117896399896), (1, 0.818571020089081)))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def key_ij(row):\n",
    "    return row[0], (row[1], row[2])\n",
    "\n",
    "\n",
    "def key_ji(row):\n",
    "    return row[1], (row[0], row[2])\n",
    "\n",
    "\n",
    "mat_join = new_mat1.map(key_ji).join(new_mat2.map(key_ij))\n",
    "mat_join.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue le produit matriciel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.01655572953207336),\n",
       " (0, 1, 0.029955235503941618),\n",
       " (1, 0, 0.22452371802884058),\n",
       " (1, 1, 0.4062437017193898),\n",
       " (2, 0, 0.12135363071412639),\n",
       " (2, 1, 0.21957211732999163),\n",
       " (3, 0, 0.38530591042420903),\n",
       " (3, 1, 0.6971561878597784),\n",
       " (4, 0, 0.0950410525952534),\n",
       " (4, 1, 0.17196325341736698),\n",
       " (5, 0, 0.001130170114032713),\n",
       " (5, 1, 0.002044881915942166)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def produit_matriciel(row):\n",
    "    index, ((i, v1), (j, v2)) = row\n",
    "    return i, j, v1 * v2\n",
    "\n",
    "\n",
    "produit = mat_join.map(produit_matriciel)\n",
    "produit.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à agréger [reduceByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey). La documentation fournit un exemple facilement transposable. Elle indique aussi : *Merge the values for each key using an associative and commutative reduce function.* Pourquoi précise-t-elle **associative et commutative** ? Cela signifie que le résultat ne dépend pas de l'ordre dans lequel l'agrégation est réalisée et qu'on peut commencer à agréger sans attendre d'avoir regroupé toutes les valeurs associées à une clé.\n",
    "\n",
    "* *Cas 1 :* [groupBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupBy) + agrégation qui commence une fois les valeurs regroupées\n",
    "* *Cas 2 :* [reduceByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey) + agrégation qui commence dès les premières valeurs regroupées\n",
    "\n",
    "Le cas 2 est moins consommateur en terme de données. Le cas 1 n'est possible que si les valeurs agrégées ne sont pas trop nombreuses. Ca tombe bien, dans notre cas, le cas 2 convient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 2.8276956385710266),\n",
       " ((0, 1), 4.344623698461026),\n",
       " ((1, 0), 2.709031589561176),\n",
       " ((1, 1), 4.286096753861166),\n",
       " ((2, 0), 2.319702595813269),\n",
       " ((2, 1), 4.014868661312692),\n",
       " ((3, 0), 2.614588815326311),\n",
       " ((3, 1), 4.66117541697328),\n",
       " ((4, 0), 1.9932138244874387),\n",
       " ((4, 1), 3.4969031578212193),\n",
       " ((5, 0), 3.0858087055402157),\n",
       " ((5, 1), 3.678401085869831),\n",
       " ((6, 0), 2.9980017336305025),\n",
       " ((6, 1), 4.739726686558273),\n",
       " ((7, 0), 3.5599644578749863),\n",
       " ((7, 1), 4.859619515198687),\n",
       " ((8, 0), 2.515794511441488),\n",
       " ((8, 1), 3.26530604000568),\n",
       " ((9, 0), 2.218822328336375),\n",
       " ((9, 1), 3.7604113556777747)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "final = produit.map(lambda row: ((row[0], row[1]), row[2])).reduceByKey(add)\n",
    "aslist = final.collect()\n",
    "aslist.sort()\n",
    "aslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat initial :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.82769564, 4.3446237 ],\n",
       "       [2.70903159, 4.28609675],\n",
       "       [2.3197026 , 4.01486866],\n",
       "       [2.61458882, 4.66117542],\n",
       "       [1.99321382, 3.49690316],\n",
       "       [3.08580871, 3.67840109],\n",
       "       [2.99800173, 4.73972669],\n",
       "       [3.55996446, 4.85961952],\n",
       "       [2.51579451, 3.26530604],\n",
       "       [2.21882233, 3.76041136]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd1 @ rnd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Même algorithme avec les Spark DataFrame\n",
    "\n",
    "On a besoin de réaliser un [flatMap](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap). Une façon de faire est de créer des colonnes qui sont de type composé : un tableau, une structure. La multiplication des lignes est obtenue avec la fonction [explode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.explode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"index\"] + [\"c%d\" % i for i in range(1, 11)]\n",
    "mat1 = spark.createDataFrame(\n",
    "    pandas.read_csv(\"rnd1.txt\", header=None, sep=\"\\t\"), schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      " |-- c3: double (nullable = true)\n",
      " |-- c4: double (nullable = true)\n",
      " |-- c5: double (nullable = true)\n",
      " |-- c6: double (nullable = true)\n",
      " |-- c7: double (nullable = true)\n",
      " |-- c8: double (nullable = true)\n",
      " |-- c9: double (nullable = true)\n",
      " |-- c10: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = [\"index\"] + [\"c%d\" % i for i in range(1, 3)]\n",
    "mat2 = spark.createDataFrame(\n",
    "    pandas.read_csv(\"rnd2.txt\", header=None, sep=\"\\t\"), schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons avoir besoin de quelques-uns des fonctions et types suivant :\n",
    "\n",
    "* [explode](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode), [posexplode](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.posexplode), [array](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.array), [alias](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias)\n",
    "* [StructType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType), [StructField](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructField)\n",
    "* [ArrayType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.ArrayType)\n",
    "* [DoubleType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType), [IntegerType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.IntegerType)\n",
    "\n",
    "Je recommande le type [FloatType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.FloatType) qui prend deux fois moins de place pour une précision moindre mais suffisante dans la plupart des cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")\n",
    "from pyspark.sql.functions import explode, posexplode, array\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- x: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"c%d\" % i for i in range(1, 11)]\n",
    "mat1_array = mat1.select(mat1.index, array(*cols).alias(\"x\"))\n",
    "mat1_array.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- pos: integer (nullable = false)\n",
      " |-- col: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat1_exploded = mat1_array.select(\"index\", posexplode(\"x\"))\n",
    "mat1_exploded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 11), (100, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1.toPandas().shape, mat1_exploded.toPandas().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recommence le même procédé pour l'autre matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"c%d\" % i for i in range(1, 3)]\n",
    "mat2_array = mat2.select(mat2.index, array(*cols).alias(\"x\"))\n",
    "mat2_exploded = mat2_array.select(\"index\", posexplode(\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à faire le produit avec la méthode [join](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) après avoir renommé les colonnes avant la jointure pour éviter les ambiguïtés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2_exp2 = (\n",
    "    mat2_exploded.withColumnRenamed(\"index\", \"index2\")\n",
    "    .withColumnRenamed(\"pos\", \"pos2\")\n",
    "    .withColumnRenamed(\"col\", \"col2\")\n",
    ")\n",
    "produit = mat1_exploded.join(mat2_exp2, mat1_exploded.pos == mat2_exp2.index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- pos: integer (nullable = false)\n",
      " |-- col: double (nullable = true)\n",
      " |-- index2: long (nullable = true)\n",
      " |-- pos2: integer (nullable = false)\n",
      " |-- col2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pos</th>\n",
       "      <th>col</th>\n",
       "      <th>index2</th>\n",
       "      <th>pos2</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036595</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036595</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496284</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496284</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  pos       col  index2  pos2      col2\n",
       "0      0    0  0.036595       0     0  0.452410\n",
       "1      0    0  0.036595       0     1  0.818571\n",
       "2      1    0  0.496284       0     0  0.452410\n",
       "3      1    0  0.496284       0     1  0.818571\n",
       "4      2    0  0.268238       0     0  0.452410"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produit.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = produit.select(\n",
    "    produit.index.alias(\"i\"),\n",
    "    produit.pos2.alias(\"j\"),\n",
    "    (produit.col * produit.col2).alias(\"val\"),\n",
    ")\n",
    "final = prod.groupby(\"i\", \"j\").sum(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i: long (nullable = true)\n",
      " |-- j: integer (nullable = false)\n",
      " |-- sum(val): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>sum(val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.827696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.344624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.709032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.286097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.319703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i  j  sum(val)\n",
       "7   0  0  2.827696\n",
       "10  0  1  4.344624\n",
       "18  1  0  2.709032\n",
       "3   1  1  4.286097\n",
       "6   2  0  2.319703"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([\"i\", \"j\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "Plutôt que d'avoir un table où chaque ligne représente trois coefficients, pourrions-nous considérer une matrice par bloc de 16x16 ? Que gagnerait-on ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "this312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
