{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices en 3 colonnes\n",
    "\n",
    "Représentation d'une matrice avec Spark / Map / Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook propose d'implémenter un produit matriciel sous Spark. Spark comme SQL n'aime pas trop avoir un nombre de colonnes variables. La première étape consiste à transformer les matrices $I\\times J$ en tableau de trois colonnes $(i,j,coefficient)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session spark with no cluster\n",
    "\n",
    "Spark est censé tourner sur un cluster. Mais ce n'est pas essentielle pour comprendre la logique. Le notebook tourne donc en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une matrice aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2962659 , 2.75236533],\n",
       "       [2.26735872, 2.89961464],\n",
       "       [1.29025917, 2.34056096],\n",
       "       [1.82876448, 3.42098919],\n",
       "       [1.91448985, 3.37298335],\n",
       "       [1.84269033, 1.98821207],\n",
       "       [2.28212544, 3.05316399],\n",
       "       [1.88631937, 3.06186776],\n",
       "       [2.67976259, 3.61823182],\n",
       "       [1.70446473, 2.71078996]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import rand\n",
    "\n",
    "rnd1 = rand(10, 10)\n",
    "rnd2 = rand(10, 2)\n",
    "rnd1 @ rnd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.425791</td>\n",
       "      <td>0.508217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.444969</td>\n",
       "      <td>0.926192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078127</td>\n",
       "      <td>0.349568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.707894</td>\n",
       "      <td>0.845050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179368</td>\n",
       "      <td>0.555457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.571995</td>\n",
       "      <td>0.419750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.782654</td>\n",
       "      <td>0.712389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.165768</td>\n",
       "      <td>0.830360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.043705</td>\n",
       "      <td>0.759277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.502934</td>\n",
       "      <td>0.110957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.425791  0.508217\n",
       "1  0.444969  0.926192\n",
       "2  0.078127  0.349568\n",
       "3  0.707894  0.845050\n",
       "4  0.179368  0.555457\n",
       "5  0.571995  0.419750\n",
       "6  0.782654  0.712389\n",
       "7  0.165768  0.830360\n",
       "8  0.043705  0.759277\n",
       "9  0.502934  0.110957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df1 = pandas.DataFrame(rnd1)\n",
    "df2 = pandas.DataFrame(rnd2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"rnd1.txt\", sep=\"\\t\", header=None, index=False)\n",
    "df2.to_csv(\"rnd2.txt\", sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = spark.sparkContext.textFile(\"rnd1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion d'une matrice au format Spark\n",
    "\n",
    "Lorsqu'un traitement est distribué en Map/Reduce, il n'est pas possible de s'appuyer sur l'ordre dans lequel sont traitées les lignes. Le plus est d'ajouter cette information sur chaque ligne plutôt que de chercher à la récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"rnd1.txt\", sep=\"\\t\", header=None, index=True)\n",
    "df2.to_csv(\"rnd2.txt\", sep=\"\\t\", header=None, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_mat_row(row):\n",
    "    values = row.split(\"\\t\")\n",
    "    index = int(values[0])\n",
    "    values = [float(_) for _ in values[1:]]\n",
    "    return [[index, j, v] for j, v in enumerate(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.03925624285714491],\n",
       " [0, 1, 0.17938143471086276],\n",
       " [0, 2, 0.9271561615741587],\n",
       " [0, 3, 0.13497740334270003],\n",
       " [0, 4, 0.20256379268518632],\n",
       " [0, 5, 0.15623553510014287],\n",
       " [0, 6, 0.7926007655892027],\n",
       " [0, 7, 0.9953375114509172],\n",
       " [0, 8, 0.7100110433596362],\n",
       " [0, 9, 0.1783301416123766],\n",
       " [1, 0, 0.6661128314162409],\n",
       " [1, 1, 0.16617620104743758]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = sc.textFile(\"rnd1.txt\")\n",
    "new_mat1 = mat1.flatMap(process_mat_row)\n",
    "new_mat1.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.4257910374269508],\n",
       " [0, 1, 0.5082167960207694],\n",
       " [1, 0, 0.4449691717763494],\n",
       " [1, 1, 0.9261922479892456],\n",
       " [2, 0, 0.07812708982401129],\n",
       " [2, 1, 0.34956810336320765],\n",
       " [3, 0, 0.7078936713530861],\n",
       " [3, 1, 0.8450500475013194],\n",
       " [4, 0, 0.17936816503487407],\n",
       " [4, 1, 0.5554570108793752],\n",
       " [5, 0, 0.5719951163381093],\n",
       " [5, 1, 0.41975047374547725]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = sc.textFile(\"rnd2.txt\")\n",
    "new_mat2 = mat2.flatMap(process_mat_row)\n",
    "new_mat2.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produit matriciel\n",
    "\n",
    "Il faut d'abord faire la jointure avec la méthode [join](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join). Il faut que la clé soit sur la première colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ((0, 0.03925624285714491), (0, 0.4257910374269508))),\n",
       " (0, ((0, 0.03925624285714491), (1, 0.5082167960207694))),\n",
       " (0, ((1, 0.6661128314162409), (0, 0.4257910374269508))),\n",
       " (0, ((1, 0.6661128314162409), (1, 0.5082167960207694))),\n",
       " (0, ((2, 0.8809481084845031), (0, 0.4257910374269508))),\n",
       " (0, ((2, 0.8809481084845031), (1, 0.5082167960207694))),\n",
       " (0, ((3, 0.09489201762862454), (0, 0.4257910374269508))),\n",
       " (0, ((3, 0.09489201762862454), (1, 0.5082167960207694))),\n",
       " (0, ((4, 0.9071785348453754), (0, 0.4257910374269508))),\n",
       " (0, ((4, 0.9071785348453754), (1, 0.5082167960207694))),\n",
       " (0, ((5, 0.8535870359856828), (0, 0.4257910374269508))),\n",
       " (0, ((5, 0.8535870359856828), (1, 0.5082167960207694)))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def key_ij(row):\n",
    "    return row[0], (row[1], row[2])\n",
    "\n",
    "\n",
    "def key_ji(row):\n",
    "    return row[1], (row[0], row[2])\n",
    "\n",
    "\n",
    "mat_join = new_mat1.map(key_ji).join(new_mat2.map(key_ij))\n",
    "mat_join.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue le produit matriciel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.016714956371628058),\n",
       " (0, 1, 0.019950681968671398),\n",
       " (1, 0, 0.2836248735321248),\n",
       " (1, 1, 0.33852972897068484),\n",
       " (2, 0, 0.37509980903092655),\n",
       " (2, 1, 0.44771262515455135),\n",
       " (3, 0, 0.04040417062962855),\n",
       " (3, 1, 0.04822571716716593),\n",
       " (4, 0, 0.3862684894832736),\n",
       " (4, 1, 0.4610433683979326),\n",
       " (5, 0, 0.3634497095865398),\n",
       " (5, 1, 0.4338072685535089)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def produit_matriciel(row):\n",
    "    index, ((i, v1), (j, v2)) = row\n",
    "    return i, j, v1 * v2\n",
    "\n",
    "\n",
    "produit = mat_join.map(produit_matriciel)\n",
    "produit.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à agréger [reduceByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey). La documentation fournit un exemple facilement transposable. Elle indique aussi : *Merge the values for each key using an associative and commutative reduce function.* Pourquoi précise-t-elle **associative et commutative** ? Cela signifie que le résultat ne dépend pas de l'ordre dans lequel l'agrégation est réalisée et qu'on peut commencer à agréger sans attendre d'avoir regroupé toutes les valeurs associées à une clé.\n",
    "\n",
    "* *Cas 1 :* [groupBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupBy) + agrégation qui commence une fois les valeurs regroupées\n",
    "* *Cas 2 :* [reduceByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey) + agrégation qui commence dès les premières valeurs regroupées\n",
    "\n",
    "Le cas 2 est moins consommateur en terme de données. Le cas 1 n'est possible que si les valeurs agrégées ne sont pas trop nombreuses. Ca tombe bien, dans notre cas, le cas 2 convient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 1.2962658962226397),\n",
       " ((0, 1), 2.7523653340144056),\n",
       " ((1, 0), 2.267358716094168),\n",
       " ((1, 1), 2.899614641397404),\n",
       " ((2, 0), 1.2902591694424805),\n",
       " ((2, 1), 2.3405609608679425),\n",
       " ((3, 0), 1.8287644824176785),\n",
       " ((3, 1), 3.420989188235977),\n",
       " ((4, 0), 1.9144898451263708),\n",
       " ((4, 1), 3.372983349186469),\n",
       " ((5, 0), 1.842690333964681),\n",
       " ((5, 1), 1.9882120730740667),\n",
       " ((6, 0), 2.282125435665258),\n",
       " ((6, 1), 3.053163988860857),\n",
       " ((7, 0), 1.8863193688285897),\n",
       " ((7, 1), 3.061867764510199),\n",
       " ((8, 0), 2.6797625884756293),\n",
       " ((8, 1), 3.6182318180423017),\n",
       " ((9, 0), 1.7044647282526524),\n",
       " ((9, 1), 2.710789957326838)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "final = produit.map(lambda row: ((row[0], row[1]), row[2])).reduceByKey(add)\n",
    "aslist = final.collect()\n",
    "aslist.sort()\n",
    "aslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat initial :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2962659 , 2.75236533],\n",
       "       [2.26735872, 2.89961464],\n",
       "       [1.29025917, 2.34056096],\n",
       "       [1.82876448, 3.42098919],\n",
       "       [1.91448985, 3.37298335],\n",
       "       [1.84269033, 1.98821207],\n",
       "       [2.28212544, 3.05316399],\n",
       "       [1.88631937, 3.06186776],\n",
       "       [2.67976259, 3.61823182],\n",
       "       [1.70446473, 2.71078996]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd1 @ rnd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Même algorithme avec les Spark DataFrame\n",
    "\n",
    "On a besoin de réaliser un [flatMap](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap). Une façon de faire est de créer des colonnes qui sont de type composé : un tableau, une structure. La multiplication des lignes est obtenue avec la fonction [explode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.explode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xadupre/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    }
   ],
   "source": [
    "schema = [\"index\"] + [\"c%d\" % i for i in range(1, 11)]\n",
    "mat1 = spark.createDataFrame(\n",
    "    pandas.read_csv(\"rnd1.txt\", header=None, sep=\"\\t\"), schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      " |-- c3: double (nullable = true)\n",
      " |-- c4: double (nullable = true)\n",
      " |-- c5: double (nullable = true)\n",
      " |-- c6: double (nullable = true)\n",
      " |-- c7: double (nullable = true)\n",
      " |-- c8: double (nullable = true)\n",
      " |-- c9: double (nullable = true)\n",
      " |-- c10: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xadupre/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    }
   ],
   "source": [
    "schema = [\"index\"] + [\"c%d\" % i for i in range(1, 3)]\n",
    "mat2 = spark.createDataFrame(\n",
    "    pandas.read_csv(\"rnd2.txt\", header=None, sep=\"\\t\"), schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons avoir besoin de quelques-uns des fonctions et types suivant :\n",
    "\n",
    "* [explode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.explode), [posexplode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.posexplode), [array](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.array), [alias](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias)\n",
    "* [StructType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.StructType), [StructField](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.StructField)\n",
    "* [ArrayType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.ArrayType)\n",
    "* [DoubleType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType), [IntegerType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.IntegerType)\n",
    "\n",
    "Je recommande le type [FloatType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.FloatType) qui prend deux fois moins de place pour une précision moindre mais suffisante dans la plupart des cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")\n",
    "from pyspark.sql.functions import explode, posexplode, array\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- x: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"c%d\" % i for i in range(1, 11)]\n",
    "mat1_array = mat1.select(mat1.index, array(*cols).alias(\"x\"))\n",
    "mat1_array.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- pos: integer (nullable = false)\n",
      " |-- col: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat1_exploded = mat1_array.select(\"index\", posexplode(\"x\"))\n",
    "mat1_exploded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 11), (100, 3))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1.toPandas().shape, mat1_exploded.toPandas().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recommence le même procédé pour l'autre matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"c%d\" % i for i in range(1, 3)]\n",
    "mat2_array = mat2.select(mat2.index, array(*cols).alias(\"x\"))\n",
    "mat2_exploded = mat2_array.select(\"index\", posexplode(\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à faire le produit avec la méthode [join](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) après avoir renommé les colonnes avant la jointure pour éviter les ambiguïtés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2_exp2 = (\n",
    "    mat2_exploded.withColumnRenamed(\"index\", \"index2\")\n",
    "    .withColumnRenamed(\"pos\", \"pos2\")\n",
    "    .withColumnRenamed(\"col\", \"col2\")\n",
    ")\n",
    "produit = mat1_exploded.join(mat2_exp2, mat1_exploded.pos == mat2_exp2.index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- pos: integer (nullable = false)\n",
      " |-- col: double (nullable = true)\n",
      " |-- index2: long (nullable = true)\n",
      " |-- pos2: integer (nullable = false)\n",
      " |-- col2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pos</th>\n",
       "      <th>col</th>\n",
       "      <th>index2</th>\n",
       "      <th>pos2</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039256</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.880948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  pos       col  index2  pos2      col2\n",
       "0      0    0  0.039256       0     0  0.425791\n",
       "1      0    0  0.039256       0     1  0.508217\n",
       "2      1    0  0.666113       0     0  0.425791\n",
       "3      1    0  0.666113       0     1  0.508217\n",
       "4      2    0  0.880948       0     0  0.425791"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produit.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = produit.select(\n",
    "    produit.index.alias(\"i\"),\n",
    "    produit.pos2.alias(\"j\"),\n",
    "    (produit.col * produit.col2).alias(\"val\"),\n",
    ")\n",
    "final = prod.groupby(\"i\", \"j\").sum(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i: long (nullable = true)\n",
      " |-- j: integer (nullable = false)\n",
      " |-- sum(val): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>sum(val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.296266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.752365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.267359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.899615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.290259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i  j  sum(val)\n",
       "7   0  0  1.296266\n",
       "10  0  1  2.752365\n",
       "18  1  0  2.267359\n",
       "3   1  1  2.899615\n",
       "6   2  0  1.290259"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([\"i\", \"j\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
