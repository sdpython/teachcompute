{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices en 3 colonnes\n",
    "\n",
    "Représentation d'une matrice avec Spark / Map / Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook propose d'implémenter un produit matriciel sous Spark. Spark comme SQL n'aime pas trop avoir un nombre de colonnes variables. La première étape consiste à transformer les matrices $I\\times J$ en tableau de trois colonnes $(i,j,coefficient)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session spark with no cluster\n",
    "\n",
    "Spark est censé tourner sur un cluster. Mais ce n'est pas essentielle pour comprendre la logique. Le notebook tourne donc en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/11/11 15:02:46 WARN Utils: Your hostname, xavier2024 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/11/11 15:02:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 15:02:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une matrice aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.87499059, 2.8278766 ],\n",
       "       [2.50319361, 2.79105007],\n",
       "       [2.47242828, 2.96505912],\n",
       "       [3.5667529 , 3.62677158],\n",
       "       [2.03659963, 1.96720702],\n",
       "       [2.45931552, 2.69251358],\n",
       "       [2.3570895 , 2.54491955],\n",
       "       [2.42398656, 3.33332776],\n",
       "       [3.2048007 , 3.60479473],\n",
       "       [2.35964323, 2.75336408]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import rand\n",
    "\n",
    "rnd1 = rand(10, 10)\n",
    "rnd2 = rand(10, 2)\n",
    "rnd1 @ rnd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534643</td>\n",
       "      <td>0.792275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868828</td>\n",
       "      <td>0.369334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015294</td>\n",
       "      <td>0.297075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.720137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.674179</td>\n",
       "      <td>0.612696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.398553</td>\n",
       "      <td>0.822608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.321764</td>\n",
       "      <td>0.881242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.173263</td>\n",
       "      <td>0.332271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.953250</td>\n",
       "      <td>0.617029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.843226</td>\n",
       "      <td>0.675595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.534643  0.792275\n",
       "1  0.868828  0.369334\n",
       "2  0.015294  0.297075\n",
       "3  0.509108  0.720137\n",
       "4  0.674179  0.612696\n",
       "5  0.398553  0.822608\n",
       "6  0.321764  0.881242\n",
       "7  0.173263  0.332271\n",
       "8  0.953250  0.617029\n",
       "9  0.843226  0.675595"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df1 = pandas.DataFrame(rnd1)\n",
    "df2 = pandas.DataFrame(rnd2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"rnd1.txt\", sep=\"\\t\", header=None, index=False)\n",
    "df2.to_csv(\"rnd2.txt\", sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = spark.sparkContext.textFile(\"rnd1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion d'une matrice au format Spark\n",
    "\n",
    "Lorsqu'un traitement est distribué en Map/Reduce, il n'est pas possible de s'appuyer sur l'ordre dans lequel sont traitées les lignes. Le plus est d'ajouter cette information sur chaque ligne plutôt que de chercher à la récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"rnd1.txt\", sep=\"\\t\", header=None, index=True)\n",
    "df2.to_csv(\"rnd2.txt\", sep=\"\\t\", header=None, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_mat_row(row):\n",
    "    values = row.split(\"\\t\")\n",
    "    index = int(values[0])\n",
    "    values = [float(_) for _ in values[1:]]\n",
    "    return [[index, j, v] for j, v in enumerate(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.05338376562534908],\n",
       " [0, 1, 0.7179621058657293],\n",
       " [0, 2, 0.8416816618000933],\n",
       " [0, 3, 0.5866463066118723],\n",
       " [0, 4, 0.2985761548971829],\n",
       " [0, 5, 0.17094084464701753],\n",
       " [0, 6, 0.22587546398891023],\n",
       " [0, 7, 0.8938702752578108],\n",
       " [0, 8, 0.7138005257890329],\n",
       " [0, 9, 0.8701336430078879],\n",
       " [1, 0, 0.9550985992178377],\n",
       " [1, 1, 0.26327652337825447]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = sc.textFile(\"rnd1.txt\")\n",
    "new_mat1 = mat1.flatMap(process_mat_row)\n",
    "new_mat1.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.5346425002633528],\n",
       " [0, 1, 0.7922752739533419],\n",
       " [1, 0, 0.8688279747342449],\n",
       " [1, 1, 0.3693339071055035],\n",
       " [2, 0, 0.01529354702055541],\n",
       " [2, 1, 0.2970747253149303],\n",
       " [3, 0, 0.5091076165917049],\n",
       " [3, 1, 0.7201371902668229],\n",
       " [4, 0, 0.674178721198942],\n",
       " [4, 1, 0.6126955081354053],\n",
       " [5, 0, 0.398552599571223],\n",
       " [5, 1, 0.8226084971982536]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = sc.textFile(\"rnd2.txt\")\n",
    "new_mat2 = mat2.flatMap(process_mat_row)\n",
    "new_mat2.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produit matriciel\n",
    "\n",
    "Il faut d'abord faire la jointure avec la méthode [join](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join). Il faut que la clé soit sur la première colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ((0, 0.05338376562534908), (0, 0.5346425002633528))),\n",
       " (0, ((0, 0.05338376562534908), (1, 0.7922752739533419))),\n",
       " (0, ((1, 0.9550985992178377), (0, 0.5346425002633528))),\n",
       " (0, ((1, 0.9550985992178377), (1, 0.7922752739533419))),\n",
       " (0, ((2, 0.16781505492071924), (0, 0.5346425002633528))),\n",
       " (0, ((2, 0.16781505492071924), (1, 0.7922752739533419))),\n",
       " (0, ((3, 0.4988373531569529), (0, 0.5346425002633528))),\n",
       " (0, ((3, 0.4988373531569529), (1, 0.7922752739533419))),\n",
       " (0, ((4, 0.2663703869883476), (0, 0.5346425002633528))),\n",
       " (0, ((4, 0.2663703869883476), (1, 0.7922752739533419))),\n",
       " (0, ((5, 0.8312264238066461), (0, 0.5346425002633528))),\n",
       " (0, ((5, 0.8312264238066461), (1, 0.7922752739533419)))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def key_ij(row):\n",
    "    return row[0], (row[1], row[2])\n",
    "\n",
    "\n",
    "def key_ji(row):\n",
    "    return row[1], (row[0], row[2])\n",
    "\n",
    "\n",
    "mat_join = new_mat1.map(key_ji).join(new_mat2.map(key_ij))\n",
    "mat_join.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue le produit matriciel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.02854122992740946),\n",
       " (0, 1, 0.04229463753548444),\n",
       " (1, 0, 0.5106363030838507),\n",
       " (1, 1, 0.7567010043477654),\n",
       " (2, 0, 0.08972106054464521),\n",
       " (2, 1, 0.13295571861080793),\n",
       " (3, 0, 0.2666996497165864),\n",
       " (3, 1, 0.3952165006305848),\n",
       " (4, 0, 0.14241292969556701),\n",
       " (4, 1, 0.21103867132425078),\n",
       " (5, 0, 0.4444089735089506),\n",
       " (5, 1, 0.6585601426386672)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def produit_matriciel(row):\n",
    "    index, ((i, v1), (j, v2)) = row\n",
    "    return i, j, v1 * v2\n",
    "\n",
    "\n",
    "produit = mat_join.map(produit_matriciel)\n",
    "produit.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à agréger [reduceByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey). La documentation fournit un exemple facilement transposable. Elle indique aussi : *Merge the values for each key using an associative and commutative reduce function.* Pourquoi précise-t-elle **associative et commutative** ? Cela signifie que le résultat ne dépend pas de l'ordre dans lequel l'agrégation est réalisée et qu'on peut commencer à agréger sans attendre d'avoir regroupé toutes les valeurs associées à une clé.\n",
    "\n",
    "* *Cas 1 :* [groupBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupBy) + agrégation qui commence une fois les valeurs regroupées\n",
    "* *Cas 2 :* [reduceByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey) + agrégation qui commence dès les premières valeurs regroupées\n",
    "\n",
    "Le cas 2 est moins consommateur en terme de données. Le cas 1 n'est possible que si les valeurs agrégées ne sont pas trop nombreuses. Ca tombe bien, dans notre cas, le cas 2 convient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 2.874990589654995),\n",
       " ((0, 1), 2.827876596857878),\n",
       " ((1, 0), 2.5031936080071473),\n",
       " ((1, 1), 2.7910500661755484),\n",
       " ((2, 0), 2.4724282791474432),\n",
       " ((2, 1), 2.9650591244671727),\n",
       " ((3, 0), 3.566752899363589),\n",
       " ((3, 1), 3.626771576425741),\n",
       " ((4, 0), 2.036599633115649),\n",
       " ((4, 1), 1.9672070245510558),\n",
       " ((5, 0), 2.459315517874097),\n",
       " ((5, 1), 2.692513578277047),\n",
       " ((6, 0), 2.3570894967638365),\n",
       " ((6, 1), 2.5449195463399703),\n",
       " ((7, 0), 2.423986556348134),\n",
       " ((7, 1), 3.333327755348771),\n",
       " ((8, 0), 3.204800695141252),\n",
       " ((8, 1), 3.6047947320158165),\n",
       " ((9, 0), 2.3596432321882945),\n",
       " ((9, 1), 2.753364084839743)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "final = produit.map(lambda row: ((row[0], row[1]), row[2])).reduceByKey(add)\n",
    "aslist = final.collect()\n",
    "aslist.sort()\n",
    "aslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat initial :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.87499059, 2.8278766 ],\n",
       "       [2.50319361, 2.79105007],\n",
       "       [2.47242828, 2.96505912],\n",
       "       [3.5667529 , 3.62677158],\n",
       "       [2.03659963, 1.96720702],\n",
       "       [2.45931552, 2.69251358],\n",
       "       [2.3570895 , 2.54491955],\n",
       "       [2.42398656, 3.33332776],\n",
       "       [3.2048007 , 3.60479473],\n",
       "       [2.35964323, 2.75336408]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd1 @ rnd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Même algorithme avec les Spark DataFrame\n",
    "\n",
    "On a besoin de réaliser un [flatMap](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap). Une façon de faire est de créer des colonnes qui sont de type composé : un tableau, une structure. La multiplication des lignes est obtenue avec la fonction [explode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.explode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"index\"] + [\"c%d\" % i for i in range(1, 11)]\n",
    "mat1 = spark.createDataFrame(\n",
    "    pandas.read_csv(\"rnd1.txt\", header=None, sep=\"\\t\"), schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      " |-- c3: double (nullable = true)\n",
      " |-- c4: double (nullable = true)\n",
      " |-- c5: double (nullable = true)\n",
      " |-- c6: double (nullable = true)\n",
      " |-- c7: double (nullable = true)\n",
      " |-- c8: double (nullable = true)\n",
      " |-- c9: double (nullable = true)\n",
      " |-- c10: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = [\"index\"] + [\"c%d\" % i for i in range(1, 3)]\n",
    "mat2 = spark.createDataFrame(\n",
    "    pandas.read_csv(\"rnd2.txt\", header=None, sep=\"\\t\"), schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons avoir besoin de quelques-uns des fonctions et types suivant :\n",
    "\n",
    "* [explode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.explode), [posexplode](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.posexplode), [array](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.functions.array), [alias](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias)\n",
    "* [StructType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.StructType), [StructField](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.StructField)\n",
    "* [ArrayType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.ArrayType)\n",
    "* [DoubleType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType), [IntegerType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.IntegerType)\n",
    "\n",
    "Je recommande le type [FloatType](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.types.FloatType) qui prend deux fois moins de place pour une précision moindre mais suffisante dans la plupart des cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")\n",
    "from pyspark.sql.functions import explode, posexplode, array\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- x: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"c%d\" % i for i in range(1, 11)]\n",
    "mat1_array = mat1.select(mat1.index, array(*cols).alias(\"x\"))\n",
    "mat1_array.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- pos: integer (nullable = false)\n",
      " |-- col: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat1_exploded = mat1_array.select(\"index\", posexplode(\"x\"))\n",
    "mat1_exploded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10, 11), (100, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1.toPandas().shape, mat1_exploded.toPandas().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recommence le même procédé pour l'autre matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"c%d\" % i for i in range(1, 3)]\n",
    "mat2_array = mat2.select(mat2.index, array(*cols).alias(\"x\"))\n",
    "mat2_exploded = mat2_array.select(\"index\", posexplode(\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à faire le produit avec la méthode [join](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) après avoir renommé les colonnes avant la jointure pour éviter les ambiguïtés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2_exp2 = (\n",
    "    mat2_exploded.withColumnRenamed(\"index\", \"index2\")\n",
    "    .withColumnRenamed(\"pos\", \"pos2\")\n",
    "    .withColumnRenamed(\"col\", \"col2\")\n",
    ")\n",
    "produit = mat1_exploded.join(mat2_exp2, mat1_exploded.pos == mat2_exp2.index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- pos: integer (nullable = false)\n",
      " |-- col: double (nullable = true)\n",
      " |-- index2: long (nullable = true)\n",
      " |-- pos2: integer (nullable = false)\n",
      " |-- col2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pos</th>\n",
       "      <th>col</th>\n",
       "      <th>index2</th>\n",
       "      <th>pos2</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.534643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.792275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.534643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.792275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.534643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  pos       col  index2  pos2      col2\n",
       "0      0    0  0.053384       0     0  0.534643\n",
       "1      0    0  0.053384       0     1  0.792275\n",
       "2      1    0  0.955099       0     0  0.534643\n",
       "3      1    0  0.955099       0     1  0.792275\n",
       "4      2    0  0.167815       0     0  0.534643"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produit.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = produit.select(\n",
    "    produit.index.alias(\"i\"),\n",
    "    produit.pos2.alias(\"j\"),\n",
    "    (produit.col * produit.col2).alias(\"val\"),\n",
    ")\n",
    "final = prod.groupby(\"i\", \"j\").sum(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i: long (nullable = true)\n",
      " |-- j: integer (nullable = false)\n",
      " |-- sum(val): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>sum(val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.874991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.827877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.503194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.791050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.472428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i  j  sum(val)\n",
       "7   0  0  2.874991\n",
       "10  0  1  2.827877\n",
       "18  1  0  2.503194\n",
       "3   1  1  2.791050\n",
       "6   2  0  2.472428"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([\"i\", \"j\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "Plutôt que d'avoir un table où chaque ligne représente trois coefficients, pourrions-nous considérer une matrice par bloc de 16x16 ? Que gagnerait-on ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
